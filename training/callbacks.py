import torch
import os
from pathlib import Path

class CheckpointManager:
    """
    Qu·∫£n l√Ω vi·ªác l∆∞u tr·ªØ v√† xoay v√≤ng (rotation) c√°c Model Checkpoint.
    
    Policy:
    - Top-k Retention: Ch·ªâ gi·ªØ l·∫°i K checkpoint t·ªët nh·∫•t d·ª±a tr√™n Validation Loss.
    - Auto-cleanup: T·ª± ƒë·ªông x√≥a c√°c file checkpoint c≈©/k√©m h∆°n ƒë·ªÉ ti·∫øt ki·ªám ·ªï c·ª©ng.
    - Metadata Saving: L∆∞u k√®m Optimizer state v√† Epoch info ƒë·ªÉ c√≥ th·ªÉ Resume training b·∫•t c·ª© l√∫c n√†o.
    """
    def __init__(self, save_dir='checkpoints', max_to_keep=3):
        self.save_dir = Path(save_dir)
        self.max_to_keep = max_to_keep
        self.best_checkpoints = []
        self.save_dir.mkdir(exist_ok=True)

    def save(self, model, optimizer, epoch, val_loss, extra_info=None):
        should_save = len(self.best_checkpoints) < self.max_to_keep or val_loss < self.best_checkpoints[-1]['loss']
        if not should_save: return

        filename = f"model_ep{epoch:03d}_loss{val_loss:.4f}.pt"
        filepath = self.save_dir / filename

        save_dict = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': val_loss,
            'extra_info': extra_info
        }
        torch.save(save_dict, filepath)
        print(f"‚úÖ Saved Top-{self.max_to_keep} Checkpoint: {filename}")

        self.best_checkpoints.append({'path': filepath, 'loss': val_loss})
        self.best_checkpoints.sort(key=lambda x: x['loss'])

        if len(self.best_checkpoints) > self.max_to_keep:
            to_remove = self.best_checkpoints.pop(-1)
            try:
                os.remove(to_remove['path'])
                print(f"üóëÔ∏è Removed old checkpoint: {to_remove['path'].name}")
            except OSError as e:
                print(f"‚ö†Ô∏è Error removing file: {e}")
