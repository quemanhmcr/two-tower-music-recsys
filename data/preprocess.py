import polars as pl
import numpy as np
from pathlib import Path
import gc
import shutil
from tqdm import tqdm
from config import TrainingConfig

def process_data():
    """
    Pipeline xá»­ lÃ½ dá»¯ liá»‡u thÃ´: Remapping ID & Cleaning.
    
    Logic chÃ­nh:
    1. Intersection: Chá»‰ giá»¯ láº¡i cÃ¡c Item ID xuáº¥t hiá»‡n trong cáº£ file Listens (hÃ nh vi) vÃ  Embeddings (content).
    2. Remapping: Ãnh xáº¡ Item ID gá»‘c (string/int lá»™n xá»™n) sang ID liÃªn tá»¥c (0 -> N-1) Ä‘á»ƒ tá»‘i Æ°u cho Embedding Layer.
    3. Consistency: Äáº£m báº£o Metadata (Artist, Album) cÅ©ng Ä‘Æ°á»£c re-map theo ID má»›i.
    
    Output:
    - CÃ¡c file Parquet Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch vÃ  Ä‘á»“ng bá»™ ID.
    - Sáºµn sÃ ng cho bÆ°á»›c táº¡o Static Features.
    """
    
    # ==============================================================================
    # CONFIGURATION
    # ==============================================================================
    cfg = TrainingConfig()
    
    OLD_DATA_DIR = cfg.DATA_ROOT / "temp_data"
    META_DATA_DIR = cfg.DATA_ROOT / "yambda_data"

    INPUT_LISTENS = OLD_DATA_DIR / "listens.parquet"
    INPUT_EMBEDDINGS = META_DATA_DIR / "embeddings.parquet"
    INPUT_ARTIST_MAP = META_DATA_DIR / "artist_item_mapping.parquet"
    INPUT_ALBUM_MAP = META_DATA_DIR / "album_item_mapping.parquet"

    OUTPUT_DIR = cfg.DATA_ROOT / "remapped_data"
    if OUTPUT_DIR.exists():
        shutil.rmtree(OUTPUT_DIR)
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    print(f"ğŸš€ STARTING ID RE-MAPPING PIPELINE (INTERSECTION LOGIC)")
    print("="*60)

    # ==============================================================================
    # STEP 1: Táº O MAPPING Tá»ª GIAO ÄIá»‚M (CRITICAL STEP)
    # ==============================================================================
    print("\n[1/5] âš”ï¸  Finding Intersection (Listens âˆ© Embeddings)...")

    # 1. QuÃ©t Unique IDs tá»« Listens
    #    (Nhá»¯ng bÃ i ngÆ°á»i dÃ¹ng Ä‘Ã£ nghe)
    q_listens_ids = pl.scan_parquet(INPUT_LISTENS).select("item_id").unique()

    # 2. QuÃ©t Unique IDs tá»« Embeddings
    #    (Nhá»¯ng bÃ i há»‡ thá»‘ng cÃ³ vector)
    #    LÆ°u Ã½: Cáº§n detect tÃªn cá»™t embedding lÃ  'item_id' hay tÃªn khÃ¡c Ä‘á»ƒ join cho Ä‘Ãºng
    q_embed_ids = pl.scan_parquet(INPUT_EMBEDDINGS).select("item_id").unique()

    # 3. Láº¥y GIAO ÄIá»‚M (INNER JOIN)
    #    Chá»‰ giá»¯ láº¡i nhá»¯ng bÃ i Vá»ªA Ä‘Æ°á»£c nghe Vá»ªA cÃ³ vector
    valid_ids = (
        q_listens_ids.join(q_embed_ids, on="item_id", how="inner")
        .collect()  # Thá»±c thi Ä‘á»ƒ láº¥y danh sÃ¡ch sáº¡ch vá» RAM
    )

    total_valid = len(valid_ids)
    print(f"   --> Found {total_valid:,} valid items (Intersection).")
    print(f"       (Items without embeddings will be dropped automatically)")

    # 4. Táº¡o Map chuáº©n (0 -> N-1)
    id_map = valid_ids.sort("item_id").with_columns(
        pl.arange(0, pl.len(), dtype=pl.UInt32).alias("new_id")
    )

    ID_MAP_FILE = OUTPUT_DIR / "id_mapping.parquet"
    id_map.write_parquet(ID_MAP_FILE)
    print(f"   --> Saved clean mapping to: {ID_MAP_FILE}")

    # Dá»n dáº¹p
    del valid_ids, q_listens_ids, q_embed_ids
    gc.collect()

    # ==============================================================================
    # STEP 2: RE-MAP LISTENS (Sáº½ tá»± Ä‘á»™ng lá»c bá» bÃ i ko cÃ³ embedding)
    # ==============================================================================
    print("\n[2/5] ğŸ§ Re-mapping 'listens.parquet'...")

    lf_listens = pl.scan_parquet(INPUT_LISTENS)
    lf_map = pl.scan_parquet(ID_MAP_FILE)

    # Inner Join á»Ÿ Ä‘Ã¢y sáº½ LOáº I Bá» nhá»¯ng dÃ²ng nghe nháº¡c mÃ  bÃ i hÃ¡t Ä‘Ã³ khÃ´ng cÃ³ trong Map
    # (tá»©c lÃ  bÃ i hÃ¡t ko cÃ³ embedding)
    lf_new_listens = (
        lf_listens.join(lf_map, on="item_id", how="inner")
        .drop("item_id")
        .rename({"new_id": "item_id"})
    )

    NEW_LISTENS_FILE = OUTPUT_DIR / "listens.parquet"
    lf_new_listens.sink_parquet(NEW_LISTENS_FILE)
    print(f"   --> âœ… Saved filtered listens to {NEW_LISTENS_FILE}")
    gc.collect()

    # ==============================================================================
    # STEP 3: RE-MAP METADATA
    # ==============================================================================
    print("\n[3/5] ğŸ“š Re-mapping Metadata...")

    def remap_metadata(input_path, output_name):
        if not input_path.exists(): return

        print(f"   ... Processing {input_path.name}")
        lf_meta = pl.scan_parquet(input_path)
        lf_map = pl.scan_parquet(ID_MAP_FILE)

        lf_new_meta = (
            lf_meta.join(lf_map, on="item_id", how="inner")
            .drop("item_id")
            .rename({"new_id": "item_id"})
        )
        lf_new_meta.sink_parquet(OUTPUT_DIR / output_name)

    remap_metadata(INPUT_ARTIST_MAP, "artist_item_mapping.parquet")
    remap_metadata(INPUT_ALBUM_MAP, "album_item_mapping.parquet")
    gc.collect()

    # ==============================================================================
    # STEP 4: RE-MAP EMBEDDINGS (Äáº£m báº£o khá»›p 100%)
    # ==============================================================================
    print("\n[4/5] ğŸ§¬ Re-mapping CNN Embeddings...")

    if INPUT_EMBEDDINGS.exists():
        schema = pl.scan_parquet(INPUT_EMBEDDINGS).schema
        emb_col = "embed" if "embed" in schema else "embedding"

        lf_emb = pl.scan_parquet(INPUT_EMBEDDINGS)
        lf_map = pl.scan_parquet(ID_MAP_FILE)

        # Join vá»›i Map (vá»‘n Ä‘Æ°á»£c táº¡o tá»« chÃ­nh embedding) -> Cháº¯c cháº¯n giá»¯ láº¡i Ä‘Ãºng sá»‘ lÆ°á»£ng
        lf_new_emb = (
            lf_emb.join(lf_map, on="item_id", how="inner")
            .select(["new_id", emb_col])
            .rename({emb_col: "embedding"})
            .sort("new_id") # Quan trá»ng: Xáº¿p Ä‘Ãºng thá»© tá»± 0, 1, 2...
        )

        NEW_EMB_FILE = OUTPUT_DIR / "embeddings.parquet"
        lf_new_emb.sink_parquet(NEW_EMB_FILE)

        # Convert to Numpy
        print("   ... Creating Numpy Matrix...")
        df_final = pl.read_parquet(NEW_EMB_FILE)
        matrix = np.stack(df_final["embedding"].to_numpy())

        NEW_EMB_NPY = OUTPUT_DIR / "embeddings_mmap.npy"
        np.save(NEW_EMB_NPY, matrix)

        print(f"   --> âœ… Matrix Shape: {matrix.shape}")

        # Validation logic
        if matrix.shape[0] == total_valid:
            print("   --> âœ… Integrity Check: Embeddings count matches Map count exactly.")
        else:
            print(f"   --> âŒ ERROR: Shape mismatch ({matrix.shape[0]} vs {total_valid})")

        del matrix, df_final
    else:
        print("   âŒ Error: Embeddings file missing.")

    gc.collect()

    # ==============================================================================
    # STEP 5: FINAL CHECK
    # ==============================================================================
    print("\n[5/5] âœ… FINAL VERIFICATION")
    # Check file listens má»›i
    df_check = pl.read_parquet(NEW_LISTENS_FILE)
    listens_unique = df_check["item_id"].n_unique()
    max_id = df_check["item_id"].max()

    print(f"Expected Unique : {total_valid:,}")
    print(f"Listens Unique  : {listens_unique:,}")
    print(f"Max ID          : {max_id:,}")

    if listens_unique == total_valid and max_id == (total_valid - 1):
        print("\nğŸ‰ SUPER SUCCESS: Listens and Embeddings are now PERFECTLY synced.")
        print(f"   Items processed: {total_valid:,}")
        print(f"   Ready for training at: {OUTPUT_DIR}")
    else:
        print("\nâŒ Still inconsistent. Check logs.")

def build_static_data():
    """
    Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u tÄ©nh (Embeddings, Artists, Albums) sang Ä‘á»‹nh dáº¡ng Numpy Dense.
    
    Optimization:
    - Sá»­ dá»¥ng `numpy.save` Ä‘á»ƒ lÆ°u binary file, cho phÃ©p load cá»±c nhanh báº±ng `mmap_mode`.
    - Chuyá»ƒn Ä‘á»•i Sparse Mapping (Parquet) sang Dense Array (Numpy) Ä‘á»ƒ truy xuáº¥t O(1) theo Item ID.
    """
    
    cfg = TrainingConfig()
    
    # CONFIG
    INPUT_DIR = cfg.DATA_ROOT / "remapped_data"  # NÆ¡i chá»©a file parquet Ä‘Ã£ map ID
    OUTPUT_DIR = cfg.STATIC_DIR # NÆ¡i lÆ°u npy

    # INPUT FILES
    FILE_EMBED = INPUT_DIR / "embeddings.parquet"
    FILE_ARTIST = INPUT_DIR / "artist_item_mapping.parquet"
    FILE_ALBUM = INPUT_DIR / "album_item_mapping.parquet"

    if OUTPUT_DIR.exists(): shutil.rmtree(OUTPUT_DIR)
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    print("ğŸš€ BUILDING STATIC FEATURES (Embeddings & Metadata)")
    print(f"   Input: {INPUT_DIR}")
    print(f"   Output: {OUTPUT_DIR}")
    print("="*60)

    # ---------------------------------------------------------
    # 1. EMBEDDINGS (Parquet -> Numpy Mmap)
    # ---------------------------------------------------------
    print("\n[1/3] ğŸ§¬ Processing Embeddings...")

    # Scan Ä‘á»ƒ láº¥y shape
    df_emb_schema = pl.scan_parquet(FILE_EMBED)
    max_id = df_emb_schema.select(pl.col("new_id").max()).collect().item()
    N_ITEMS = max_id + 1
    EMBED_DIM = 128 # Giáº£ Ä‘á»‹nh, hoáº·c láº¥y len cá»§a vector Ä‘áº§u tiÃªn

    print(f"   Total Items: {N_ITEMS:,}")

    # Load toÃ n bá»™ vÃ o RAM (vá»›i 2.5M items * 128 float32 ~ 1.2GB RAM -> Kháº£ thi trÃªn Colab)
    # Náº¿u RAM yáº¿u, dÃ¹ng cÃ¡ch chunking nhÆ° mÃ£ cÅ© cá»§a báº¡n.
    print("   Loading embeddings into RAM (fast method)...")
    df_emb = pl.read_parquet(FILE_EMBED).sort("new_id")

    # Kiá»ƒm tra cá»™t vector tÃªn gÃ¬
    col_name = "embedding" if "embedding" in df_emb.columns else "embed"

    # Convert sang Numpy Matrix
    matrix = np.stack(df_emb[col_name].to_numpy())

    # Save
    np.save(OUTPUT_DIR / "embeddings.npy", matrix)
    print(f"   âœ… Saved embeddings.npy {matrix.shape}")

    del df_emb, matrix
    gc.collect()

    # ---------------------------------------------------------
    # 2. ARTIST MAP (Sparse -> Dense Array)
    # ---------------------------------------------------------
    print("\n[2/3] ğŸ¤ Processing Artist Map...")

    # Táº¡o máº£ng chá»©a toÃ n sá»‘ 0 (Unknown)
    # DÃ¹ng int32 Ä‘á»ƒ tiáº¿t kiá»‡m (náº¿u < 2 tá»· artist)
    artist_dense = np.zeros(N_ITEMS, dtype=np.int32)

    if FILE_ARTIST.exists():
        df_art = pl.read_parquet(FILE_ARTIST)
        # item_id chÃ­nh lÃ  index trong máº£ng dense
        indices = df_art["item_id"].to_numpy()
        values = df_art["artist_id"].to_numpy()

        # GÃ¡n giÃ¡ trá»‹
        artist_dense[indices] = values
        count = len(indices)
    else:
        print("   âš ï¸ No artist file found, array will be zeros.")
        count = 0

    np.save(OUTPUT_DIR / "artists.npy", artist_dense)
    print(f"   âœ… Saved artists.npy (Filled {count:,}/{N_ITEMS:,} items)")
    del artist_dense

    # ---------------------------------------------------------
    # 3. ALBUM MAP (Sparse -> Dense Array)
    # ---------------------------------------------------------
    print("\n[3/3] ğŸ’¿ Processing Album Map...")

    album_dense = np.zeros(N_ITEMS, dtype=np.int32)

    if FILE_ALBUM.exists():
        df_alb = pl.read_parquet(FILE_ALBUM)
        indices = df_alb["item_id"].to_numpy()
        values = df_alb["album_id"].to_numpy()

        album_dense[indices] = values
        count = len(indices)
    else:
        print("   âš ï¸ No album file found, array will be zeros.")
        count = 0

    np.save(OUTPUT_DIR / "albums.npy", album_dense)
    print(f"   âœ… Saved albums.npy (Filled {count:,}/{N_ITEMS:,} items)")
    del album_dense

    print("\nğŸ‰ Static Data Build Complete!")

def build_interactions():
    """
    XÃ¢y dá»±ng cáº¥u trÃºc dá»¯ liá»‡u tÆ°Æ¡ng tÃ¡c (Interactions) tá»‘i Æ°u cho Random Access.
    
    Architecture:
    - Sharding: Chia nhá» dá»¯ liá»‡u thÃ nh nhiá»u partition dá»±a trÃªn User ID hash Ä‘á»ƒ xá»­ lÃ½ song song vÃ  trÃ¡nh trÃ n RAM.
    - Sorting: Sáº¯p xáº¿p dá»¯ liá»‡u theo (User, Time) Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh tuáº§n tá»± thá»i gian.
    - Flattening: Gá»™p táº¥t cáº£ partition thÃ nh 2 máº£ng pháº³ng khá»•ng lá»“ (Items, Timestamps) lÆ°u trÃªn Ä‘Ä©a (Memory Mapped).
    - Indexing: Táº¡o máº£ng `offsets` Ä‘á»ƒ trá» Ä‘áº¿n vá»‹ trÃ­ báº¯t Ä‘áº§u/káº¿t thÃºc cá»§a tá»«ng User trong máº£ng pháº³ng.
    
    Káº¿t quáº£: Truy xuáº¥t lá»‹ch sá»­ cá»§a User báº¥t ká»³ chá»‰ tá»‘n O(1) disk seek.
    """
    
    cfg = TrainingConfig()
    
    # CONFIGURATION
    INPUT_LISTENS = cfg.DATA_ROOT / "remapped_data/listens.parquet"
    OUTPUT_DIR = cfg.INTERACTIONS_DIR
    TEMP_DIR = cfg.DATA_ROOT / "temp_partitions"

    # Cáº¤U HÃŒNH TÃŠN Cá»˜T (ÄÃ£ sá»­a theo log lá»—i cá»§a báº¡n)
    COL_USER = "uid"        # <--- Sá»¬A Tá»ª 'user_id' THÃ€NH 'uid'
    COL_ITEM = "item_id"
    COL_TIME = "timestamp"

    NUM_PARTITIONS = 50

    def check_schema():
        """Kiá»ƒm tra tÃªn cá»™t trÆ°á»›c khi cháº¡y Ä‘á»ƒ trÃ¡nh lá»—i giá»¯a chá»«ng"""
        print("ğŸ” Checking Schema...")
        try:
            schema = pl.scan_parquet(INPUT_LISTENS).limit(1).collect().columns
            print(f"   Detected Columns: {schema}")

            required = [COL_USER, COL_ITEM, COL_TIME]
            missing = [col for col in required if col not in schema]

            if missing:
                print(f"âŒ ERROR: Missing columns in parquet file: {missing}")
                print(f"   Please update CONFIG variables in the script.")
                return False
            return True
        except Exception as e:
            print(f"âŒ ERROR reading file: {e}")
            return False

    # 1. Check Schema trÆ°á»›c
    if not check_schema():
        return

    # 2. Setup thÆ° má»¥c
    if OUTPUT_DIR.exists(): shutil.rmtree(OUTPUT_DIR)
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    if TEMP_DIR.exists(): shutil.rmtree(TEMP_DIR)
    TEMP_DIR.mkdir(parents=True, exist_ok=True)

    print("\nğŸš€ BUILDING INTERACTION ARRAYS (CSR Style)")
    print("="*60)

    # ---------------------------------------------------------
    # PHASE 1: SHARDING (Chia nhá» file Ä‘á»ƒ sort)
    # ---------------------------------------------------------
    print("\n[1/3] ğŸ”ª Partitioning data by User ID...")

    # Äáº¿m tá»•ng dÃ²ng
    total_rows = pl.scan_parquet(INPUT_LISTENS).select(pl.count()).collect().item()
    print(f"   Total Interactions: {total_rows:,}")

    # Loop qua cÃ¡c partition
    for i in tqdm(range(NUM_PARTITIONS), desc="Partitioning"):
        # Filter & Select
        lf = pl.scan_parquet(INPUT_LISTENS).filter(
            (pl.col(COL_USER).hash(seed=42) % NUM_PARTITIONS) == i
        )

        # Select cá»™t cáº§n thiáº¿t vÃ  cast kiá»ƒu
        df_part = lf.select([
            pl.col(COL_USER).cast(pl.UInt32),
            pl.col(COL_ITEM).cast(pl.UInt32),
            pl.col(COL_TIME).cast(pl.UInt32),
        ]).collect()

        if df_part.height > 0:
            # Sort ngay táº¡i Ä‘Ã¢y (Quan trá»ng: Sort theo UID trÆ°á»›c, rá»“i Ä‘áº¿n Time)
            df_part = df_part.sort([COL_USER, COL_TIME])
            df_part.write_parquet(TEMP_DIR / f"part_{i:03d}.parquet")

        del df_part
        gc.collect()

    # ---------------------------------------------------------
    # PHASE 2: ALLOCATE MEMMAP (Táº¡o file rá»—ng trÃªn Ä‘Ä©a)
    # ---------------------------------------------------------
    print("\n[2/3] ğŸ’¾ Allocating memory-mapped files...")

    # Pre-allocate file kÃ­ch thÆ°á»›c lá»›n trÃªn á»• cá»©ng
    mmap_items = np.memmap(OUTPUT_DIR / "flat_item_ids.npy", dtype='uint32', mode='w+', shape=(total_rows,))
    mmap_times = np.memmap(OUTPUT_DIR / "flat_timestamps.npy", dtype='uint32', mode='w+', shape=(total_rows,))

    # List táº¡m chá»©a Ä‘á»™ dÃ i lá»‹ch sá»­ cá»§a tá»«ng user
    user_lengths = []

    # ---------------------------------------------------------
    # PHASE 3: MERGE & FLATTEN
    # ---------------------------------------------------------
    print("\n[3/3] ğŸšœ Merging partitions into flat arrays...")

    current_offset = 0
    partition_files = sorted(TEMP_DIR.glob("*.parquet"))

    for p_file in tqdm(partition_files, desc="Merging"):
        df = pl.read_parquet(p_file)

        # Láº¥y máº£ng numpy ra (SiÃªu nhanh)
        arr_users = df[COL_USER].to_numpy()
        arr_items = df[COL_ITEM].to_numpy()
        arr_times = df[COL_TIME].to_numpy()

        # 1. Copy Data vÃ o Mmap
        n_rows = len(df)
        mmap_items[current_offset : current_offset + n_rows] = arr_items
        mmap_times[current_offset : current_offset + n_rows] = arr_times
        current_offset += n_rows

        # 2. TÃ­nh User Group Lengths
        # VÃ¬ data Ä‘Ã£ sort theo user, ta dÃ¹ng np.unique Ä‘á»ƒ Ä‘áº¿m sá»‘ dÃ²ng cá»§a má»—i user
        # return_counts tráº£ vá» sá»‘ interaction cá»§a tá»«ng user
        # LÆ°u Ã½: VÃ¬ ta chia partition theo hash user, nÃªn 1 user CHáº®C CHáº®N chá»‰ náº±m trá»n váº¹n trong 1 partition
        _, counts = np.unique(arr_users, return_counts=True)

        # counts chÃ­nh lÃ  length history cá»§a tá»«ng user trong partition nÃ y
        user_lengths.extend(counts)

        del df, arr_users, arr_items, arr_times
        gc.collect()

    # Flush data xuá»‘ng Ä‘Ä©a (Save)
    mmap_items.flush()
    mmap_times.flush()

    # TÃ­nh User Offsets
    print("   Calculating User Offsets...")
    user_lengths = np.array(user_lengths, dtype=np.uint32)

    # Offsets lÃ  máº£ng tÃ­ch lÅ©y: [0, len_u1, len_u1+len_u2, ...]
    offsets = np.zeros(len(user_lengths) + 1, dtype=np.uint64)
    offsets[1:] = np.cumsum(user_lengths)

    np.save(OUTPUT_DIR / "user_offsets.npy", offsets)

    print(f"âœ… DONE!")
    print(f"   Total Users : {len(user_lengths):,}")
    print(f"   Output Files: {OUTPUT_DIR}")

    # XÃ³a temp Ä‘á»ƒ giáº£i phÃ³ng á»• cá»©ng
    shutil.rmtree(TEMP_DIR)

if __name__ == "__main__":
    process_data()
    build_static_data()
    build_interactions()
