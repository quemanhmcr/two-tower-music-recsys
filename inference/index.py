import faiss
import torch
import numpy as np
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
import gc
from ..data.dataset import FastItemInferenceDataset

def optimized_collate_fn(batch):
    """Custom collate function"""
    embeds, artists, albums, ids = zip(*batch)
    return {
        # [FIX CRITICAL] ƒê·∫£m b·∫£o Tensor l√† Float32
        "item_embed": torch.from_numpy(np.stack(embeds)).to(torch.float32),
        "item_artist_id": torch.tensor(artists, dtype=torch.long),
        "item_album_id": torch.tensor(albums, dtype=torch.long),
        "target_item_id": torch.tensor(ids, dtype=torch.long)
    }

def generate_and_index(trainer, config):
    """
    Quy tr√¨nh Inference & Indexing t·ªïng th·ªÉ.
    
    Workflow:
    1. Embedding Generation: Ch·∫°y Item Tower (Forward Pass) cho to√†n b·ªô t·∫≠p Item ƒë·ªÉ sinh ra vector ƒë·∫°i di·ªán.
    2. FAISS Indexing: ƒê∆∞a c√°c vector n√†y v√†o c·∫•u tr√∫c d·ªØ li·ªáu FAISS (Facebook AI Similarity Search) ƒë·ªÉ ph·ª•c v·ª• t√¨m ki·∫øm vector t·ªëc ƒë·ªô cao.
    3. Serialization: L∆∞u Index xu·ªëng ƒëƒ©a ƒë·ªÉ d√πng cho Serving API.
    
    Performance Note:
    - S·ª≠ d·ª•ng `torch.inference_mode()` v√† `autocast` ƒë·ªÉ t·ªëi ƒëa h√≥a t·ªëc ƒë·ªô sinh vector.
    - Batch Size l·ªõn (16k) gi√∫p b√£o h√≤a GPU Core.
    """
    print(f"\nüöÄ STARTING HIGH-PERFORMANCE INFERENCE...")

    # Clean Memory
    if hasattr(trainer, 'optimizer'): del trainer.optimizer
    if hasattr(trainer, 'scaler'): del trainer.scaler
    gc.collect()
    torch.cuda.empty_cache()

    # Config
    BATCH_SIZE = 16384
    NUM_WORKERS = 2

    dataset = FastItemInferenceDataset(config)
    dataloader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=NUM_WORKERS,
        collate_fn=optimized_collate_fn,
        pin_memory=True,
        prefetch_factor=2,
        persistent_workers=True
    )

    # Model & Device Setup
    model = trainer.model.item_tower
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Auto-move model to GPU
    if next(model.parameters()).device.type != 'cuda':
        print(f"‚ö†Ô∏è Model is on CPU. Moving to {device}...")
        model = model.to(device)
    else:
        print(f"‚úÖ Model is already on {device}.")

    model.eval()

    # Output Buffer
    final_embeddings = np.zeros((len(dataset), config.EMBED_DIM), dtype=np.float32)
    print(f"   üî• Processing with Batch Size: {BATCH_SIZE}")

    current_idx = 0

    with torch.inference_mode():
        with torch.amp.autocast('cuda'):
            for batch in tqdm(dataloader, desc="‚ö° Inferencing"):
                # Move to GPU
                batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}

                # Forward Pass
                outputs = model(batch)

                # Copy results
                batch_len = outputs.shape[0]
                final_embeddings[current_idx : current_idx + batch_len] = outputs.cpu().numpy()
                current_idx += batch_len

    print(f"‚úÖ Inference Complete. Shape: {final_embeddings.shape}")

    # Build FAISS
    print(f"\nüèóÔ∏è BUILDING FAISS INDEX (Exact Search)...")
    index = faiss.IndexFlatIP(config.EMBED_DIM)
    index.add(final_embeddings)

    save_path = f"{config.CHECKPOINT_DIR}/item_vectors_256d.faiss"
    faiss.write_index(index, save_path)

    print(f"üéâ SUCCESS! Saved to: {save_path}")
    return index, final_embeddings
